import pandas as pd, numpy as np, joblib, os, xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, classification_report
from sklearn.impute import SimpleImputer
from pathlib import Path

# ê²½ë¡œ
BASE_DIR = Path(f"./")
DATA_PATH = "C:/Users/mstot/PycharmProjects/PythonProject1/card_segment/data/preprocessing.csv"
MODEL_DIR = Path(f"./")
TARGET, ID_COL = "Segment.1", "ID"

MODEL_DIR.mkdir(parents=True, exist_ok=True)

# ë°ì´í„° ë¡œë“œ (ì´ë¯¸ ì¸ì½”ë”©Â·ìºìŠ¤íŒ… ì™„ë£Œ ìƒíƒœ)
df = pd.read_csv(DATA_PATH)

# í•™ìŠµ í”¼ì²˜ / ë ˆì´ë¸”
X_full = df.drop(columns=[TARGET, ID_COL]).astype(np.float32)  # float32 í†µì¼
# â”€â”€ ìˆ«ìí˜•ë§Œ ì„ íƒ (ë¬¸ìí˜• ë³€í™˜ ë°©ì§€) â”€â”€â”€â”€â”€â”€â”€â”€
X_full = X_full.select_dtypes(include=[np.number]).astype(np.float32)
y_full = df[TARGET].str.upper()  # 'A'~'E'

# (Stage, Positive Label, Remaining Labels) - A, BëŠ” ë³„ë„ ì²˜ë¦¬
STAGES = [
    ("E", {"E"}),  # Stage-1 : E vs not-E
    ("D", {"D"}),  # Stage-2 : D vs not-D
    ("C", {"C"}),  # Stage-3 : C vs not-C
    # A, BëŠ” ë³„ë„ AB ì „ìš© ëª¨ë¸ë¡œ ì²˜ë¦¬
]


def train_binary_stage(X, y, pos_label, *,
                       n_splits=5, random_state=42):
    # â”€â”€ 1) ë°”ì´ë„ˆë¦¬ ë¼ë²¨ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    y_bin = (y == pos_label).astype(int)  # 1 = positive
    pos_cnt = y_bin.sum()
    neg_cnt = len(y_bin) - pos_cnt

    # â”€â”€ 2) scale_pos_weight : í•­ìƒ ì†Œìˆ˜ í´ë˜ìŠ¤ > 1 â”€â”€â”€â”€â”€â”€â”€â”€
    if pos_cnt == 0:  # ì˜ˆì™¸ ë°©ì§€
        spw = 1.0
    elif pos_cnt < neg_cnt:  # Positive ê°€ minority â†’ ê°€ì¤‘ì¹˜ â†‘
        spw = neg_cnt / pos_cnt
    else:  # Positive ê°€ majority â†’ ê°€ì¤‘ì¹˜ 1
        spw = 1.0

    params = dict(
        objective="binary:logistic",
        tree_method="hist",  # GPU
        device="cuda",
        max_bin=1024,
        learning_rate=0.08,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=spw,  # â† ìˆ˜ì •ëœ ê°’
        eval_metric="aucpr",
        base_score=0.5,
    )

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True,
                          random_state=random_state)

    # OOF ë²„í¼
    oof_proba = np.zeros(len(X), dtype=np.float32)

    models, f1_scores = [], []
    for fold, (tr, va) in enumerate(skf.split(X, y_bin)):
        dtrain = xgb.DMatrix(X.iloc[tr], label=y_bin.iloc[tr])
        dvalid = xgb.DMatrix(X.iloc[va], label=y_bin.iloc[va])

        model = xgb.train(
            params, dtrain,
            num_boost_round=10_000,
            early_stopping_rounds=200,
            evals=[(dvalid, "val")],
            verbose_eval=300,
        )
        models.append(model)

        proba = model.predict(dvalid)
        oof_proba[va] = proba  # OOF ì €ì¥

        f1 = f1_score(y_bin.iloc[va],
                      (proba > 0.5).astype(int))
        f1_scores.append(f1)
        print(f"[Fold {fold}] {pos_label} F1 = {f1:.4f}")

    print(f"â–¶ {pos_label} Mean F1 = {np.mean(f1_scores):.4f}\n")
    return models, oof_proba, y_bin.values


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Precision ëª©í‘œì¹˜ ëŒ€í­ ì™„í™” (ë” ë§ì€ ë°ì´í„°ê°€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ë„ë¡)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sklearn.metrics import precision_recall_curve
import joblib, numpy as np, pandas as pd, os

#  Precision ëª©í‘œì¹˜ ëŒ€í­ ì™„í™” - Recall í™•ë³´ë¥¼ ìœ„í•´
PREC_TARGET = {"E": 0.93,  # 0.998 â†’ 0.95 (ëŒ€í­ ì™„í™”)
               "D": 0.90,  # 0.985 â†’ 0.92 (ëŒ€í­ ì™„í™”)
               "C": 0.88,  # 0.970 â†’ 0.90 (ëŒ€í­ ì™„í™”)
               }

stage_models = {}
thr_dict = {}
stage_stats = {}  # {label: TPÂ·FPÂ·â€¦}
oof_buf = {}  # {label: {"idx":â€¦, "proba":â€¦}}

X_stage, y_stage = X_full, y_full.copy()
remain_idx_global = np.arange(len(df))  # ì „ì²´ DF ì¸ë±ìŠ¤ì™€ ë™ê¸°í™”

for label, _ in STAGES:
    print(f"\n========== Stage {label} ==========")

    # 1) í•™ìŠµ & OOF í™•ë¥ 
    models, oof_p, y_bin = train_binary_stage(X_stage, y_stage, label)
    stage_models[label] = models
    joblib.dump(models, MODEL_DIR / f"stage_{label}.pkl")

    # 2) cut-off : Precision â‰¥ ëª©í‘œì¹˜ ì¤‘ Recall ìµœëŒ“ê°’ ì§€ì  (F1 fallback ì¼ê´€ ì ìš©)
    p, r, th = precision_recall_curve(y_bin, oof_p)

    # precisionÂ·recall ì˜ ì²« ê°’(p=1, r=0) ì œê±° â†’ ê¸¸ì´ë¥¼ thresholds ì™€ ë§ì¶¤
    p_aligned = p[1:]
    r_aligned = r[1:]

    mask_prec = p_aligned >= PREC_TARGET[label]

    if mask_prec.any():
        idx_best = r_aligned[mask_prec].argmax()
        best_thr = th[mask_prec][idx_best]
        print(f"   â†³ Precision ëª©í‘œ ë‹¬ì„±: P={p_aligned[mask_prec][idx_best]:.4f}, R={r_aligned[mask_prec][idx_best]:.4f}")
    else:
        #  F1 fallback ì¼ê´€ ì ìš© - Precision ëª©í‘œ ë¯¸ë‹¬ ì‹œ
        f1_curve = 2 * p_aligned * r_aligned / (p_aligned + r_aligned + 1e-9)
        if len(f1_curve) > 0 and np.isfinite(f1_curve).any():
            best_idx = f1_curve.argmax()
            best_thr = th[best_idx]
            print(
                f"   â†³ F1 fallback ì ìš©: P={p_aligned[best_idx]:.4f}, R={r_aligned[best_idx]:.4f}, F1={f1_curve[best_idx]:.4f}")
        else:
            best_thr = 0.5
            print(f"   â†³ ê¸°ë³¸ê°’ ì‚¬ìš©: thr=0.5")

    thr_dict[label] = float(best_thr)
    print(f"   â†³ best_thr({label}) = {best_thr:.4f}  "
          f"(target P={PREC_TARGET[label]:.3f})")

    # 3) ìŠ¤í…Œì´ì§€ë³„ í˜¼ë™í–‰ë ¬ ì§‘ê³„
    pred_pos = oof_p > best_thr
    tp = int(((pred_pos) & (y_bin == 1)).sum())
    fp = int(((pred_pos) & (y_bin == 0)).sum())
    fn = int(((~pred_pos) & (y_bin == 1)).sum())
    tn = int(((~pred_pos) & (y_bin == 0)).sum())

    prec = tp / (tp + fp + 1e-9)
    rec = tp / (tp + fn + 1e-9)
    f1 = 2 * prec * rec / (prec + rec + 1e-9)

    stage_stats[label] = dict(TP=tp, FP=fp, FN=fn, TN=tn,
                              precision=prec, recall=rec, F1=f1)

    # 4) OOF í™•ë¥  & ì¸ë±ìŠ¤ ì €ì¥  (2ì°¨ Cascade, ìµœì¢… OOF í‰ê°€ìš©)
    oof_buf[label] = {"idx": remain_idx_global.copy(),
                      "proba": oof_p.astype(np.float32)}

    # 5) ë‹¤ìŒ ìŠ¤í…Œì´ì§€ìš© ë°ì´í„° ì¶•ì†Œ (negative = not-Positive)
    mask_neg = ~pred_pos
    remain_idx_global = remain_idx_global[mask_neg]
    X_stage = X_stage.iloc[mask_neg]
    y_stage = y_stage.iloc[mask_neg]

    print(f"   â†³ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ëŠ” ìƒ˜í”Œ ìˆ˜: {len(X_stage)}")

# 6) threshold ì €ì¥
joblib.dump(thr_dict, MODEL_DIR / "stage_thresholds.pkl")
joblib.dump(oof_buf, MODEL_DIR / "stage_oof_prob.pkl")
print("ğŸ’¾ ëª¨ë¸ Â· threshold Â· OOF í™•ë¥  ì €ì¥ ì™„ë£Œ")

# 7) ë‹¨ê³„ë³„ ì§€í‘œ ì¶œë ¥
stats_df = (
    pd.DataFrame(stage_stats)
    .T[["TP", "FP", "FN", "TN", "precision", "recall", "F1"]]
    .round({"precision": 4, "recall": 4, "F1": 4})
)
print("\n=== Stage-wise Confusion / Metrics ===")
print(stats_df)

# ========================================================
#  A, B ì „ìš© ëª¨ë¸ (ê°œì„ : ê²°ì¸¡ì¹˜ ì²˜ë¦¬ + ì˜¤ë²„ìƒ˜í”Œë§)
# ========================================================
from imblearn.over_sampling import SMOTE, ADASYN
from sklearn.model_selection import StratifiedKFold


def train_ab_specialized_model(X_full, y_full, random_state=42):
    """A, Bë§Œ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” ì „ìš© ëª¨ë¸ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬ + ì˜¤ë²„ìƒ˜í”Œë§ ì ìš©)"""
    print(f"\n========== A, B ì „ìš© ëª¨ë¸ í•™ìŠµ ==========")

    # A, B í´ë˜ìŠ¤ë§Œ ì¶”ì¶œ
    mask_ab = y_full.isin(['A', 'B'])
    X_ab = X_full[mask_ab].reset_index(drop=True)
    y_ab = y_full[mask_ab].reset_index(drop=True)

    #  ì „ë¶€ NaNì¸ ì»¬ëŸ¼ ì œê±° (Imputer ì—ëŸ¬ ë°©ì§€)
    X_ab = X_ab.dropna(axis=1, how='all')

    print(f"A, B ì›ë³¸ ë¶„í¬:")
    print(y_ab.value_counts())

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (SMOTE ì˜¤ë¥˜ ë°©ì§€)
    print(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ shape: {X_ab.shape}")
    print(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {X_ab.isnull().sum().sum()}")

    if X_ab.isnull().sum().sum() > 0:
        print("ê²°ì¸¡ì¹˜ ë°œê²¬ - SimpleImputerë¡œ ì²˜ë¦¬")
        imputer = SimpleImputer(strategy='median')
        X_ab = pd.DataFrame(imputer.fit_transform(X_ab),
                            columns=X_ab.columns,
                            index=X_ab.index)
        print(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„: {X_ab.isnull().sum().sum()}ê°œ")

    # ì˜¤ë²„ìƒ˜í”Œë§ ì ìš©
    try:
        # k_neighborsë¥¼ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ì„¤ì •
        min_class_size = min(y_ab.value_counts())
        k_neighbors = min(5, max(1, min_class_size - 1))

        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)
        X_ab_resampled, y_ab_resampled = smote.fit_resample(X_ab, y_ab)
        print(f"SMOTE ì„±ê³µ! k_neighbors={k_neighbors}")
        print(f"SMOTE í›„ ë¶„í¬:")
        print(pd.Series(y_ab_resampled).value_counts())
    except Exception as e:
        print(f"SMOTE ì‹¤íŒ¨: {e}")
        print("ì›ë³¸ ë°ì´í„°ë¡œ í•™ìŠµ ì§„í–‰")
        X_ab_resampled, y_ab_resampled = X_ab, y_ab

    # A vs B ë°”ì´ë„ˆë¦¬ ëª¨ë¸ í•™ìŠµ (A=1, B=0)
    y_ab_bin = (y_ab_resampled == 'A').astype(int)

    pos_cnt = y_ab_bin.sum()
    neg_cnt = len(y_ab_bin) - pos_cnt
    spw = neg_cnt / pos_cnt if pos_cnt > 0 else 1.0

    params = dict(
        objective="binary:logistic",
        tree_method="hist",
        device="cuda",
        max_bin=1024,
        learning_rate=0.05,  # ë” ì„¸ë°€í•œ í•™ìŠµ
        max_depth=8,  # ë” ê¹Šì€ íŠ¸ë¦¬
        subsample=0.85,
        colsample_bytree=0.85,
        scale_pos_weight=spw,
        eval_metric="aucpr",
        base_score=0.5,
        reg_alpha=0.1,  # L1 ì •ê·œí™”
        reg_lambda=0.1,  # L2 ì •ê·œí™”
    )

    # ì›ë³¸ A, B ë°ì´í„°ì— ëŒ€í•œ OOF ì˜ˆì¸¡
    # CV splits ìˆ˜ ì¡°ì • (ìµœì†Œ í´ë˜ìŠ¤ í¬ê¸° ê³ ë ¤)
    n_splits = min(5, min(y_ab.value_counts()))
    if n_splits < 2:
        n_splits = 2

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    oof_proba_ab = np.zeros(len(X_ab), dtype=np.float32)
    ab_models = []

    for fold, (tr, va) in enumerate(skf.split(X_ab, y_ab)):
        # í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì˜¤ë²„ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ ì‚¬ìš©
        dtrain = xgb.DMatrix(X_ab_resampled, label=y_ab_bin)
        dvalid = xgb.DMatrix(X_ab.iloc[va], label=(y_ab.iloc[va] == 'A').astype(int))

        model = xgb.train(
            params, dtrain,
            num_boost_round=3000,  # ìƒ˜í”Œ ìˆ˜ê°€ ì ìœ¼ë¯€ë¡œ ì¤„ì„
            early_stopping_rounds=100,
            evals=[(dvalid, "val")],
            verbose_eval=200,
        )
        ab_models.append(model)

        proba = model.predict(dvalid)
        oof_proba_ab[va] = proba

        # Foldë³„ ì„±ëŠ¥
        y_pred_fold = (proba > 0.5).astype(int)
        y_true_fold = (y_ab.iloc[va] == 'A').astype(int)
        f1_fold = f1_score(y_true_fold, y_pred_fold, zero_division=0)
        print(f"[AB Fold {fold}] F1 = {f1_fold:.4f}")

    #  Threshold ìµœì í™” (F1 Score ê¸°ì¤€ - ì¼ê´€ì„± ìœ ì§€)
    y_ab_bin_orig = (y_ab == 'A').astype(int)

    # Precision-Recall curve ê¸°ë°˜ threshold ê³„ì‚°
    try:
        p, r, th = precision_recall_curve(y_ab_bin_orig, oof_proba_ab)
        p_aligned = p[1:]
        r_aligned = r[1:]

        # F1 ìµœëŒ€í™” ë°©ì‹ ì‚¬ìš© (AB ëª¨ë¸ì€ íŠ¹ë³„íˆ precision íƒ€ê²Ÿ ì—†ìŒ)
        f1_curve = 2 * p_aligned * r_aligned / (p_aligned + r_aligned + 1e-9)
        if len(f1_curve) > 0 and np.isfinite(f1_curve).any():
            best_idx = f1_curve.argmax()
            best_thr_ab = th[best_idx]
            print(f"   â†³ AB ëª¨ë¸ F1 ê¸°ì¤€ ìµœì  threshold = {best_thr_ab:.4f}")
            print(f"   â†³ ìµœì ì : P={p_aligned[best_idx]:.4f}, R={r_aligned[best_idx]:.4f}, F1={f1_curve[best_idx]:.4f}")
        else:
            best_thr_ab = 0.5
            print(f"   â†³ AB ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©: thr=0.5")
    except Exception as e:
        print(f"   â†³ AB threshold ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        best_thr_ab = 0.5

    # ì„±ëŠ¥ í‰ê°€
    pred_ab = (oof_proba_ab > best_thr_ab).astype(int)
    tp_ab = int(((pred_ab == 1) & (y_ab_bin_orig == 1)).sum())
    fp_ab = int(((pred_ab == 1) & (y_ab_bin_orig == 0)).sum())
    fn_ab = int(((pred_ab == 0) & (y_ab_bin_orig == 1)).sum())
    tn_ab = int(((pred_ab == 0) & (y_ab_bin_orig == 0)).sum())

    prec_ab = tp_ab / (tp_ab + fp_ab + 1e-9)
    rec_ab = tp_ab / (tp_ab + fn_ab + 1e-9)
    f1_ab = 2 * prec_ab * rec_ab / (prec_ab + rec_ab + 1e-9)

    ab_stats = dict(TP=tp_ab, FP=fp_ab, FN=fn_ab, TN=tn_ab,
                    precision=prec_ab, recall=rec_ab, F1=f1_ab)

    # ì›ë³¸ ì¸ë±ìŠ¤ ë³µì›
    ab_indices = np.where(mask_ab)[0]

    return ab_models, best_thr_ab, ab_stats, {
        "idx": ab_indices,
        "proba": oof_proba_ab.astype(np.float32)
    }


# A, B ì „ìš© ëª¨ë¸ í•™ìŠµ
ab_models, ab_threshold, ab_stats, ab_oof_buf = train_ab_specialized_model(X_full, y_full)

# A, B ëª¨ë¸ ì €ì¥
joblib.dump(ab_models, MODEL_DIR / "ab_specialized_model.pkl")
joblib.dump({"threshold": ab_threshold}, MODEL_DIR / "ab_threshold.pkl")
joblib.dump(ab_oof_buf, MODEL_DIR / "ab_oof_prob.pkl")

print(f"\n=== A, B ì „ìš© ëª¨ë¸ ì„±ëŠ¥ ===")
print(f"Precision: {ab_stats['precision']:.4f}")
print(f"Recall: {ab_stats['recall']:.4f}")
print(f"F1: {ab_stats['F1']:.4f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  2ì°¨ Cascade (E, D, Cë§Œ)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# LightGBM í•™ìŠµ í•¨ìˆ˜ ì •ì˜ (íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜ í•´ê²° + F1 fallback ì¼ê´€ ì ìš©)
def train_lgb_stage(X, y, pos_label, *, n_splits=5, random_state=42):
    """LightGBM ê¸°ë°˜ ë°”ì´ë„ˆë¦¬ ë¶„ë¥˜ í•™ìŠµ (íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜ í•´ê²° + F1 fallback ì¼ê´€ ì ìš©)"""
    try:
        import lightgbm as lgb
    except ImportError:
        print("âš ï¸ LightGBM not installed. ì„¤ì¹˜ í•„ìš”: pip install lightgbm")
        return [], np.zeros(len(X), dtype=np.float32), 0.5, np.zeros(len(X))

    from sklearn.metrics import precision_recall_curve

    # ğŸ”¥ íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜ í•´ê²°: ì»¬ëŸ¼ëª… ì •ë¦¬
    X_clean = X.copy()
    X_clean.columns = X_clean.columns.str.replace(r"[^\w]", "_", regex=True)
    X_clean.columns = X_clean.columns.str.replace(r"_+", "_", regex=True)
    X_clean.columns = X_clean.columns.str.strip("_")

    print(f"   â†³ ì»¬ëŸ¼ëª… ì •ë¦¬ ì™„ë£Œ: {X.shape[1]}ê°œ í”¼ì²˜")

    # ë°”ì´ë„ˆë¦¬ ë¼ë²¨ ìƒì„±
    y_bin = (y == pos_label).astype(int)
    pos_cnt = y_bin.sum()
    neg_cnt = len(y_bin) - pos_cnt

    if pos_cnt == 0:
        print(f"âš ï¸ {pos_label}: ì–‘ì„± ìƒ˜í”Œ ì—†ìŒ, ë¹ˆ ëª¨ë¸ ë°˜í™˜")
        return [], np.zeros(len(X), dtype=np.float32), 0.5, y_bin.values

    # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì²´í¬
    if len(X) < 10:
        print(f"âš ï¸ {pos_label}: ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ ({len(X)}ê°œ), ë¹ˆ ëª¨ë¸ ë°˜í™˜")
        return [], np.zeros(len(X), dtype=np.float32), 0.5, y_bin.values

    # CV splits ì¡°ì •
    actual_splits = min(n_splits, min(pos_cnt, neg_cnt))
    if actual_splits < 2:
        actual_splits = 2
        print(f"âš ï¸ {pos_label}: CV splitsë¥¼ {actual_splits}ë¡œ ì¡°ì •")

    # scale_pos_weight ê³„ì‚°
    spw = neg_cnt / pos_cnt if pos_cnt < neg_cnt else 1.0

    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': min(31, max(10, len(X) // 20)),
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'scale_pos_weight': spw,
        'verbose': -1,
        'random_state': random_state,
        'force_col_wise': True,
        'min_data_in_leaf': max(1, len(X) // 100)
    }

    try:
        skf = StratifiedKFold(n_splits=actual_splits, shuffle=True, random_state=random_state)
        oof_proba = np.zeros(len(X), dtype=np.float32)
        models = []

        for fold, (tr, va) in enumerate(skf.split(X_clean, y_bin)):
            # ë°ì´í„° ê²€ì¦
            if len(tr) == 0 or len(va) == 0:
                print(f"âš ï¸ {pos_label} Fold {fold}: ë¹ˆ í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸")
                continue

            # ê° foldì—ì„œ í´ë˜ìŠ¤ ê· í˜• í™•ì¸
            if y_bin.iloc[tr].sum() == 0 or y_bin.iloc[va].sum() == 0:
                print(f"âš ï¸ {pos_label} Fold {fold}: í•œìª½ í´ë˜ìŠ¤ë§Œ ì¡´ì¬")
                oof_proba[va] = 0.5
                continue

            train_data = lgb.Dataset(
                X_clean.iloc[tr],
                label=y_bin.iloc[tr],
                free_raw_data=False
            )
            valid_data = lgb.Dataset(
                X_clean.iloc[va],
                label=y_bin.iloc[va],
                reference=train_data,
                free_raw_data=False
            )

            early_stopping_rounds = min(100, max(10, 1000 // actual_splits))

            model = lgb.train(
                params, train_data,
                num_boost_round=1000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                callbacks=[
                    lgb.early_stopping(early_stopping_rounds),
                    lgb.log_evaluation(0)
                ]
            )
            models.append(model)

            # ì˜ˆì¸¡ ì‹œ ì•ˆì „ì¥ì¹˜
            try:
                proba = model.predict(X_clean.iloc[va], num_iteration=model.best_iteration)
                proba = np.where(np.isfinite(proba), proba, 0.5)
                proba = np.clip(proba, 1e-7, 1 - 1e-7)
                oof_proba[va] = proba
            except Exception as e:
                print(f"âš ï¸ {pos_label} Fold {fold} ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                oof_proba[va] = 0.5

        if len(models) == 0:
            print(f"âš ï¸ {pos_label}: í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŒ")
            return [], np.full(len(X), 0.5, dtype=np.float32), 0.5, y_bin.values

        #  Threshold ê³„ì‚° (Precision ê¸°ì¤€ + F1 fallback ì¼ê´€ ì ìš©)
        prec_target = PREC_TGT2.get(pos_label, 0.9)

        try:
            p, r, th = precision_recall_curve(y_bin, oof_proba)
            p_aligned, r_aligned = p[1:], r[1:]

            mask_prec = p_aligned >= prec_target
            if mask_prec.any():
                best_idx = r_aligned[mask_prec].argmax()
                best_thr = th[mask_prec][best_idx]
                print(
                    f"   â†³ LGB {pos_label}: Precision ëª©í‘œ ë‹¬ì„± - P={p_aligned[mask_prec][best_idx]:.4f}, R={r_aligned[mask_prec][best_idx]:.4f}")
            else:
                #  F1 fallback ì¼ê´€ ì ìš©
                f1_curve = 2 * p_aligned * r_aligned / (p_aligned + r_aligned + 1e-9)
                if len(f1_curve) > 0 and np.isfinite(f1_curve).any():
                    best_idx = f1_curve.argmax()
                    best_thr = th[best_idx]
                    print(
                        f"   â†³ LGB {pos_label}: F1 fallback ì ìš© - P={p_aligned[best_idx]:.4f}, R={r_aligned[best_idx]:.4f}, F1={f1_curve[best_idx]:.4f}")
                else:
                    best_thr = 0.5
                    print(f"   â†³ LGB {pos_label}: ê¸°ë³¸ê°’ ì‚¬ìš© - thr=0.5")
        except Exception as e:
            print(f"âš ï¸ {pos_label}: Threshold ê³„ì‚° ì˜¤ë¥˜: {e}")
            best_thr = 0.5

        print(f"   â†³ LGB {pos_label}: thr={best_thr:.4f}, target_P={prec_target:.3f}")
        return models, oof_proba, best_thr, y_bin.values

    except Exception as e:
        print(f"âš ï¸ {pos_label}: LightGBM í•™ìŠµ ì „ì²´ ì˜¤ë¥˜: {e}")
        return [], np.full(len(X), 0.5, dtype=np.float32), 0.5, np.zeros(len(X))


##  2ì°¨ Cascadeìš© Precision ëª©í‘œì¹˜ (ë” ì—„ê²©í•˜ê²Œ ì„¤ì •)
PREC_TGT2 = {"E": 0.90,  # 2ì°¨ì—ì„œëŠ” ë” ë†’ì€ ì •ë°€ë„
             "D": 0.85,
             "C": 0.83}

# 2ì°¨ Cascade ëª¨ë¸ í•™ìŠµ (LightGBM)
stage_models_2nd = {}
thr_dict_2nd = {}
stage_stats_2nd = {}
oof_buf_2nd = {}

# 1ì°¨ì—ì„œ negativeë¡œ ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤ë§Œ ì‚¬ìš©
print(f"\n========== 2ì°¨ Cascade í•™ìŠµ ì‹œì‘ ==========")

# 1ì°¨ì—ì„œ ë‚¨ì€ ìƒ˜í”Œë“¤ (E, D, Cê°€ ëª¨ë‘ negativeë¡œ ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤)
# ì „ì²´ ë°ì´í„°ì—ì„œ 1ì°¨ ìŠ¤í…Œì´ì§€ë³„ë¡œ positive ì˜ˆì¸¡ëœ ìƒ˜í”Œë“¤ ì œì™¸
excluded_indices = set()

for label in ["E", "D", "C"]:
    stage_oof = oof_buf[label]
    stage_thr = thr_dict[label]
    pos_mask = stage_oof["proba"] > stage_thr
    pos_indices = stage_oof["idx"][pos_mask]
    excluded_indices.update(pos_indices)

# 2ì°¨ í•™ìŠµìš© ë°ì´í„° (1ì°¨ì—ì„œ ëª¨ë“  ë‹¨ê³„ë¥¼ í†µê³¼í•œ ìƒ˜í”Œë“¤)
remaining_indices = np.array([i for i in range(len(df)) if i not in excluded_indices])
X_2nd = X_full.iloc[remaining_indices].reset_index(drop=True)
y_2nd = y_full.iloc[remaining_indices].reset_index(drop=True)

print(f"2ì°¨ Cascade í•™ìŠµ ë°ì´í„°: {len(X_2nd)}ê°œ ìƒ˜í”Œ")
print(f"2ì°¨ ë ˆì´ë¸” ë¶„í¬:\n{y_2nd.value_counts()}")

# 2ì°¨ Cascade ë‹¨ê³„ë³„ í•™ìŠµ
X_stage_2nd, y_stage_2nd = X_2nd, y_2nd.copy()
remain_idx_2nd = remaining_indices.copy()

for label, _ in STAGES:
    if len(X_stage_2nd) == 0:
        print(f"âš ï¸ {label} 2ì°¨: í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        continue

    print(f"\n---------- 2ì°¨ {label} ë‹¨ê³„ ----------")

    # LightGBMìœ¼ë¡œ í•™ìŠµ
    models_2nd, oof_p_2nd, best_thr_2nd, y_bin_2nd = train_lgb_stage(
        X_stage_2nd, y_stage_2nd, label
    )

    if len(models_2nd) == 0:
        print(f"âš ï¸ {label} 2ì°¨: ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        continue

    stage_models_2nd[label] = models_2nd
    thr_dict_2nd[label] = float(best_thr_2nd)

    # ì„±ëŠ¥ í‰ê°€
    pred_pos_2nd = oof_p_2nd > best_thr_2nd
    tp_2nd = int(((pred_pos_2nd) & (y_bin_2nd == 1)).sum())
    fp_2nd = int(((pred_pos_2nd) & (y_bin_2nd == 0)).sum())
    fn_2nd = int(((~pred_pos_2nd) & (y_bin_2nd == 1)).sum())
    tn_2nd = int(((~pred_pos_2nd) & (y_bin_2nd == 0)).sum())

    prec_2nd = tp_2nd / (tp_2nd + fp_2nd + 1e-9)
    rec_2nd = tp_2nd / (tp_2nd + fn_2nd + 1e-9)
    f1_2nd = 2 * prec_2nd * rec_2nd / (prec_2nd + rec_2nd + 1e-9)

    stage_stats_2nd[label] = dict(TP=tp_2nd, FP=fp_2nd, FN=fn_2nd, TN=tn_2nd,
                                  precision=prec_2nd, recall=rec_2nd, F1=f1_2nd)

    # OOF ì €ì¥
    oof_buf_2nd[label] = {"idx": remain_idx_2nd.copy(),
                          "proba": oof_p_2nd.astype(np.float32)}

    # ë‹¤ìŒ ë‹¨ê³„ìš© ë°ì´í„° ì¶•ì†Œ
    mask_neg_2nd = ~pred_pos_2nd
    remain_idx_2nd = remain_idx_2nd[mask_neg_2nd]
    X_stage_2nd = X_stage_2nd.iloc[mask_neg_2nd].reset_index(drop=True)
    y_stage_2nd = y_stage_2nd.iloc[mask_neg_2nd].reset_index(drop=True)

    print(f"   â†³ 2ì°¨ {label} ì™„ë£Œ: P={prec_2nd:.4f}, R={rec_2nd:.4f}, F1={f1_2nd:.4f}")
    print(f"   â†³ ë‹¤ìŒ ë‹¨ê³„ ìƒ˜í”Œ ìˆ˜: {len(X_stage_2nd)}")

# 2ì°¨ ëª¨ë¸ ì €ì¥
joblib.dump(stage_models_2nd, MODEL_DIR / "stage_models_2nd.pkl")
joblib.dump(thr_dict_2nd, MODEL_DIR / "stage_thresholds_2nd.pkl")
joblib.dump(oof_buf_2nd, MODEL_DIR / "stage_oof_prob_2nd.pkl")

# 2ì°¨ ê²°ê³¼ ì¶œë ¥
if stage_stats_2nd:
    stats_df_2nd = (
        pd.DataFrame(stage_stats_2nd)
        .T[["TP", "FP", "FN", "TN", "precision", "recall", "F1"]]
        .round({"precision": 4, "recall": 4, "F1": 4})
    )
    print("\n=== 2ì°¨ Cascade Stage-wise Metrics ===")
    print(stats_df_2nd)

# ============================================================
#  ìµœì¢… OOF ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± (1ì°¨ + 2ì°¨ + AB ê²°í•©)
# ============================================================

print(f"\n========== ìµœì¢… OOF ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± ==========")

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ìµœì¢… ì˜ˆì¸¡ ì´ˆê¸°í™”
final_predictions = pd.Series(['UNKNOWN'] * len(df), index=df.index)

# 1) 1ì°¨ Cascade ê²°ê³¼ ì ìš©
for label in ["E", "D", "C"]:
    if label in oof_buf:
        stage_oof = oof_buf[label]
        stage_thr = thr_dict[label]
        pos_mask = stage_oof["proba"] > stage_thr
        pos_indices = stage_oof["idx"][pos_mask]
        final_predictions.iloc[pos_indices] = label
        print(f"1ì°¨ {label}: {len(pos_indices)}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡")

# 2) 2ì°¨ Cascade ê²°ê³¼ ì ìš© (1ì°¨ì—ì„œ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ìƒ˜í”Œë“¤ ì¤‘ì—ì„œ)
for label in ["E", "D", "C"]:
    if label in oof_buf_2nd:
        stage_oof_2nd = oof_buf_2nd[label]
        stage_thr_2nd = thr_dict_2nd[label]
        pos_mask_2nd = stage_oof_2nd["proba"] > stage_thr_2nd
        pos_indices_2nd = stage_oof_2nd["idx"][pos_mask_2nd]

        # ì•„ì§ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ìƒ˜í”Œë“¤ë§Œ ì—…ë°ì´íŠ¸
        unassigned_mask = final_predictions.iloc[pos_indices_2nd] == 'UNKNOWN'
        final_pos_indices = pos_indices_2nd[unassigned_mask]
        final_predictions.iloc[final_pos_indices] = label
        print(f"2ì°¨ {label}: {len(final_pos_indices)}ê°œ ì¶”ê°€ ìƒ˜í”Œ ì˜ˆì¸¡")

# 3) A, B ì „ìš© ëª¨ë¸ ê²°ê³¼ ì ìš©
if ab_oof_buf and ab_threshold:
    ab_indices = ab_oof_buf["idx"]
    ab_proba = ab_oof_buf["proba"]

    # A vs B ì˜ˆì¸¡ (A=1, B=0)
    pred_A = ab_proba > ab_threshold
    pred_B = ~pred_A

    final_predictions.iloc[ab_indices[pred_A]] = 'A'
    final_predictions.iloc[ab_indices[pred_B]] = 'B'
    print(f"AB ëª¨ë¸: A={pred_A.sum()}ê°œ, B={pred_B.sum()}ê°œ ìƒ˜í”Œ ì˜ˆì¸¡")

# 4) ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ìƒ˜í”Œë“¤ ì²˜ë¦¬ (ê¸°ë³¸ê°’ í• ë‹¹)
unassigned_mask = final_predictions == 'UNKNOWN'
unassigned_count = unassigned_mask.sum()

if unassigned_count > 0:
    print(f"âš ï¸ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ìƒ˜í”Œ: {unassigned_count}ê°œ")

    # ì‹¤ì œ ë ˆì´ë¸” ë¶„í¬ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ê°’ í• ë‹¹
    actual_dist = y_full.value_counts(normalize=True)
    most_common_label = actual_dist.index[0]

    final_predictions[unassigned_mask] = most_common_label
    print(f"   â†³ ê¸°ë³¸ê°’ '{most_common_label}' í• ë‹¹")

# 5) ìµœì¢… ê²°ê³¼ í‰ê°€
print(f"\n=== ìµœì¢… OOF ì˜ˆì¸¡ ê²°ê³¼ ===")
print(f"ì˜ˆì¸¡ ë¶„í¬:")
print(final_predictions.value_counts())

print(f"\nì‹¤ì œ ë¶„í¬:")
print(y_full.value_counts())

# Classification Report
from sklearn.metrics import classification_report, confusion_matrix

# 1. ë¬¸ìì—´ ìºìŠ¤íŒ…
y_true = y_full.astype(str)
y_pred = final_predictions.astype(str)

# 2. classification report
print("\n=== ìµœì¢… Classification Report ===")
print(classification_report(y_true, y_pred, zero_division=0))

# 3. confusion matrix
cm = confusion_matrix(y_true, y_pred, labels=sorted(y_true.unique()))
cm_df = pd.DataFrame(cm,
                     index=[f"True_{l}" for l in sorted(y_true.unique())],
                     columns=[f"Pred_{l}" for l in sorted(y_true.unique())])
print("\n=== ìµœì¢… Confusion Matrix ===")
print(cm_df)

# print(f"\n=== ìµœì¢… Classification Report ===")
# print(classification_report(y_full, final_predictions, zero_division=0))
#
# # Confusion Matrix
# print(f"\n=== ìµœì¢… Confusion Matrix ===")
# cm = confusion_matrix(y_full, final_predictions)
# cm_df = pd.DataFrame(cm,
#                      index=[f"True_{label}" for label in sorted(y_full.unique())],
#                      columns=[f"Pred_{label}" for label in sorted(final_predictions.unique())])
# print(cm_df)

# ì „ì²´ ì •í™•ë„
overall_accuracy = (final_predictions == y_full).mean()
print(f"\nì „ì²´ ì •í™•ë„: {overall_accuracy:.4f}")

# í´ë˜ìŠ¤ë³„ ì •í™•ë„
class_accuracy = {}
for label in y_full.unique():
    mask = y_full == label
    if mask.sum() > 0:
        acc = (final_predictions[mask] == label).mean()
        class_accuracy[label] = acc

print(f"\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:")
for label, acc in sorted(class_accuracy.items()):
    print(f"  {label}: {acc:.4f}")

# ìµœì¢… ê²°ê³¼ ì €ì¥
result_df = pd.DataFrame({
    'ID': df[ID_COL],
    'Actual': y_full,
    'Predicted': final_predictions
})
result_df.to_csv(MODEL_DIR / "final_oof_predictions.csv", index=False)

print(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {MODEL_DIR / 'final_oof_predictions.csv'}")
print(f"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

# ============================================================
# ğŸ”¥ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
# ============================================================

print(f"\n" + "=" * 60)
print(f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
print(f"=" * 60)

print(f"\n 1ì°¨ Cascade (XGBoost)")
if not stats_df.empty:
    print(stats_df)

print(f"\n 2ì°¨ Cascade (LightGBM)")
if 'stats_df_2nd' in locals() and not stats_df_2nd.empty:
    print(stats_df_2nd)

print(f"\n A, B ì „ìš© ëª¨ë¸")
print(f"Precision: {ab_stats['precision']:.4f}")
print(f"Recall: {ab_stats['recall']:.4f}")
print(f"F1: {ab_stats['F1']:.4f}")

print(f"\n ìµœì¢… í†µí•© ì„±ëŠ¥")
print(f"ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f}")
print(f"í´ë˜ìŠ¤ë³„ ì •í™•ë„: {dict(sorted(class_accuracy.items()))}")

print(f"\n ì €ì¥ëœ íŒŒì¼ë“¤:")
saved_files = [
    "stage_E.pkl", "stage_D.pkl", "stage_C.pkl",
    "stage_thresholds.pkl", "stage_oof_prob.pkl",
    "ab_specialized_model.pkl", "ab_threshold.pkl", "ab_oof_prob.pkl",
    "stage_models_2nd.pkl", "stage_thresholds_2nd.pkl", "stage_oof_prob_2nd.pkl",
    "final_oof_predictions.csv"
]

for file in saved_files:
    file_path = MODEL_DIR / file
    if file_path.exists():
        print(f"âœ… {file}")
    else:
        print(f"âŒ {file} (ëˆ„ë½)")

print(f"\nğŸ¯ ì™„ë£Œ! ëª¨ë“  ëª¨ë¸ê³¼ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

